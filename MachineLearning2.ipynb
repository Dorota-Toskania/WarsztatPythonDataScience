{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2\n",
    "\n",
    "https://github.com/MichalKorzycki/WarsztatPythonDataScience\n",
    "\n",
    "Plik: `MachineLearning2.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### - Łańcuchy przetwarzania - Pipelines w sklearn\n",
    "#### - Charakterystyka danych tekstowych\n",
    "#### - Przestrzeń do rozwoju\n",
    "#### - Trenowanie, walidacja i testowanie modeli\n",
    "- Proces budowy modelu\n",
    "- Walidacja krzyżowa\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "https://www.gumtree.pl/a-mieszkania-i-domy-sprzedam-i-kupie/praga-polnoc/mieszkanie-inwestycyjne-4+pok-przy-metrze-wilenska-targowa-70/1007172232370910500042709\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import log2\n",
    "\n",
    "data = pd.read_csv('adverts_29_04.csv', sep=';')\n",
    "data['cena_za_metr'] = data['Cena'] / data['Wielkość (m2)']\n",
    "data = data.dropna(subset=['cena_za_metr'])\n",
    "df = data.drop(['Cena', 'Data dodania'], axis=1)\n",
    "dum_df = pd.get_dummies(df, columns=['Lokalizacja', 'Na sprzedaż przez', 'Rodzaj nieruchomości', 'Liczba pokoi', 'Liczba łazienek', 'Parking'])\n",
    "dum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "y = dum_df['cena_za_metr']\n",
    "X = dum_df.drop(['cena_za_metr', 'opis'], axis=1)\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "reg.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pipeline = Pipeline(\n",
    "[ ('scaler', StandardScaler()),  ('linear', LinearRegression()) ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = dum_df['cena_za_metr']\n",
    "X = dum_df.drop(['cena_za_metr', 'opis'], axis=1)\n",
    "\n",
    "reg = pipeline.fit(X, y)\n",
    "reg.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co z opisem ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import sys\n",
    "import re\n",
    "\n",
    "f = gzip.open('odm.txt.gz', 'rt', encoding='utf-8')\n",
    "dictionary = {}\n",
    "\n",
    "for x in f:\n",
    "    t = x.strip().split(',')\n",
    "    tt = [ x.strip().lower() for x in t]\n",
    "    for w in tt[1:]: \n",
    "        dictionary[w]=tt[0]\n",
    "\n",
    "def lematize(w):\n",
    "    w = w.replace('ą','ą')\n",
    "    w = w.replace('ó','ó')\n",
    "    w = w.replace('ę','ę')\n",
    "    w = w.replace('ż','ż')\n",
    "    return dictionary.get(w,w)\n",
    "\n",
    "opis1 = dum_df['opis'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "splitter = re.compile(r'[^ąąćęńłóóśśżżź\\w]+')\n",
    "isnumber = re.compile(r'[0-9]')\n",
    "\n",
    "\n",
    "def preprocessing(opis):\n",
    "    opis = str(opis)\n",
    "    tokenized = splitter.split(opis)\n",
    "    l = list(tokenized)\n",
    "    l = [ x.lower() for x in l ]\n",
    "    l = [ x for x in l if isnumber.search(x) is None ]\n",
    "    l = [ lematize(x) for x in l ]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus=[]\n",
    "n=0\n",
    "\n",
    "for i in dum_df.iterrows():\n",
    "    n+=1\n",
    "    l = list(splitter.split(i[1][1]))\n",
    "    raw_corpus.append(l)\n",
    "\n",
    "    \n",
    "all_words = []\n",
    "for t in raw_corpus:\n",
    "    all_words[0:0] = t\n",
    "\n",
    "print(f'Słów: {len(all_words)} z {n} dokumentów')\n",
    "    \n",
    "words = {}\n",
    "for w in all_words:\n",
    "    rec = words.get(w.lower(), {'upper':0, 'lower': 0})\n",
    "    if w.lower()==w or w.upper()==w:\n",
    "        rec['lower'] = rec['lower'] +1\n",
    "    else: \n",
    "        rec['upper'] = rec['upper'] +1\n",
    "    words[w.lower()] = rec\n",
    "\n",
    "print(len(words))\n",
    "\n",
    "raw_stop_words = [ x for x in words.keys() if words[x]['upper']>=words[x]['lower']*8 ]   \n",
    "print(len(raw_stop_words))\n",
    "print(raw_stop_words[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "splitter = re.compile(r'[^ąąćęńłóóśśżżź\\w]+')\n",
    "isnumber = re.compile(r'[0-9]')\n",
    "\n",
    "set_raw_stop_words = set(raw_stop_words)\n",
    "\n",
    "def preprocessing(opis):\n",
    "    opis = str(opis)\n",
    "    tokenized = splitter.split(opis)\n",
    "    l = list(tokenized)\n",
    "    l = [ x.lower() for x in l ]\n",
    "    l = [ x for x in l if len(x) > 2]\n",
    "    l = [ x for x in l if isnumber.search(x) is None ]\n",
    "    l = [ x for x in l if x not in set_raw_stop_words ]\n",
    "    l = [ lematize(x) for x in l ]\n",
    "    l = [ x for x in l if len(x) > 2]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opis1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessing(opis1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for i in dum_df.iterrows():\n",
    "    l = preprocessing(i[1][1])\n",
    "    corpus.append(l)\n",
    "\n",
    "print(f\"Mamy {len(corpus)} tekstów\")\n",
    "\n",
    "all_words = []\n",
    "for t in corpus:\n",
    "    all_words += t\n",
    " \n",
    "print(f\"Mamy {len(all_words)} wyrazów\")\n",
    "all_words[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = {}\n",
    "\n",
    "for w in all_words:\n",
    "    counter[w] = counter.get(w,0)+1\n",
    "\n",
    "print(f\"Mamy {len(counter.keys())} RÓŻNYCH wyrazów\")\n",
    "counted_words= [ (word,cnt) for word,cnt in counter.items() ]\n",
    "counted_words[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "counted_words.sort(key=itemgetter(1), reverse=True)\n",
    "counted_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [ x[1] for x in counted_words ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(counts[:140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = pd.DataFrame(counts[:140])\n",
    "count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "plt.figure(figsize=(24,12))\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "chart = sns.scatterplot(\n",
    "                     color='purple', \n",
    "                     data=count_df\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metryka TF-IDF\n",
    "ile razy występuję wyraz *i* w tekście *j*\n",
    "$${n}_{ij}$$ \n",
    " ### Term Frequency (TF)\n",
    " \n",
    " $${tf}_{ij} = \\frac{{n}_{ij}}{\\sum{k}{{n}_{ik}}}$$\n",
    " \n",
    " W tekście *j* sprawdzamy ile proporcjonalnie do całości występuje wyraz *i*\n",
    "### Inverted Document Frequency (IDF)\n",
    "\n",
    " $$idf_i = log \\frac{|D|}{ \\{ d: n_i \\in d \\}}$$\n",
    " \n",
    " licznik - liczba dokumentów\n",
    " \n",
    " mianownik - liczba dokumentów w którym wystapił wyraz *i*-ty "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline dla tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_df[\"opis\"] = dum_df[\"opis\"].apply(lambda x: ' '.join(preprocessing(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                ('selector', ItemSelector(key='opis')),\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('best', TruncatedSVD(n_components=120)),\n",
    "                ('linear', LinearRegression())\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dum_df['cena_za_metr']\n",
    "X = dum_df.drop(['cena_za_metr'], axis=1)\n",
    "\n",
    "reg = pipeline.fit(X, y)\n",
    "\n",
    "reg.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "                              \n",
    "class ItemUnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict.drop([self.key], axis=1)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling features from the post's subject line\n",
    "            ('table', Pipeline([\n",
    "                ('selector', ItemUnSelector(key='opis')),\n",
    "            ])),\n",
    "\n",
    "            # Pipeline for standard bag-of-words model for body\n",
    "            ('description', Pipeline([\n",
    "                ('selector', ItemSelector(key='opis')),\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('best', TruncatedSVD(n_components=120)),\n",
    "            ]))\n",
    "        ],\n",
    "\n",
    "        # weight components in FeatureUnion\n",
    "        transformer_weights={\n",
    "            'table': 1.0,\n",
    "            'description': 1.0,\n",
    "        },\n",
    "    )),\n",
    "\n",
    "    # Use a SVC classifier on the combined features\n",
    "    ('linear', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dum_df['cena_za_metr']\n",
    "X = dum_df.drop(['cena_za_metr'], axis=1)\n",
    "\n",
    "reg = pipeline.fit(X, y)\n",
    "\n",
    "reg.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Przestrzeń do rozwoju\n",
    "- Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import log2\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "data = pd.read_csv('adverts_29_04.csv', sep=';')\n",
    "data['cena_za_metr'] = data['Cena'] / data['Wielkość (m2)']\n",
    "data = data.dropna(subset=['cena_za_metr'])\n",
    "df = data.drop(['Cena', 'Data dodania'], axis=1)\n",
    "dum_df = pd.get_dummies(df, columns=['Lokalizacja', 'Na sprzedaż przez', 'Rodzaj nieruchomości', 'Liczba pokoi', 'Liczba łazienek', 'Parking'])\n",
    "\n",
    "\n",
    "y = dum_df['cena_za_metr']\n",
    "X = dum_df.drop(['cena_za_metr', 'opis'], axis=1)\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "reg.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import log2\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "data = pd.read_csv('adverts_29_04.csv', sep=';')\n",
    "data['cena_za_metr'] = data['Cena'] / data['Wielkość (m2)']\n",
    "data[\"log\"] = data['Wielkość (m2)'].apply(lambda x: log2(x))\n",
    "data = data.dropna(subset=['cena_za_metr'])\n",
    "df = data.drop(['Cena', 'Data dodania'], axis=1)\n",
    "dum_df = pd.get_dummies(df, columns=['Lokalizacja', 'Na sprzedaż przez', 'Rodzaj nieruchomości', 'Liczba pokoi', 'Liczba łazienek', 'Parking'])\n",
    "\n",
    "\n",
    "y = dum_df['cena_za_metr']\n",
    "X = dum_df.drop(['cena_za_metr', 'opis'], axis=1)\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "reg.score(X,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optymalizacja hiper parametrów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                ('selector', ItemSelector(key='opis')),\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('best', TruncatedSVD(n_components=120)),\n",
    "                ('linear', LinearRegression())\n",
    "            ])\n",
    "\n",
    "y = dum_df['cena_za_metr']\n",
    "X = dum_df.drop(['cena_za_metr'], axis=1)\n",
    "\n",
    "reg = pipeline.fit(X, y)\n",
    "\n",
    "reg.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                ('selector', ItemSelector(key='opis')),\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('best', TruncatedSVD(n_components=250)),\n",
    "                ('linear', LinearRegression())\n",
    "            ])\n",
    "\n",
    "y = dum_df['cena_za_metr']\n",
    "X = dum_df.drop(['cena_za_metr'], axis=1)\n",
    "\n",
    "reg = pipeline.fit(X, y)\n",
    "\n",
    "reg.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pipeline = Pipeline(\n",
    "[ ('scaler', Normalizer()),  ('linear', LinearRegression()) ]\n",
    ")\n",
    "\n",
    "y = dum_df['cena_za_metr']\n",
    "X = dum_df.drop(['cena_za_metr', 'opis'], axis=1)\n",
    "\n",
    "reg = pipeline.fit(X, y)\n",
    "reg.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... ale to na kolejnym spotkaniu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Trenowanie, testowanie, walidacja\n",
    "\n",
    "- Bład który popełniamy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.style.use(\"dark_background\")\n",
    "x = np.linspace(-2, 2, 100)\n",
    "plt.plot(x, x+0.6*x*x)\n",
    "plt.plot(x, 1.3*x)\n",
    "x = np.linspace(-2, 2, 10)\n",
    "plt.scatter(x, x+0.5*np.abs(x));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Oddzielmy trenowanie od walidacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=0)\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "reg = LinearRegression(normalize=True).fit(X_train, y_train)\n",
    "reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Trenowanie, walidacja i testowanie modeli\n",
    "\n",
    "### Proces budowy modelu\n",
    "\n",
    "1. Dzielimy dane na zbiór _**trenujący**_ (np. 75%), zbiór  _**walidacyjny**_  (np. 15%), zbiór _**testowy**_ (np. 10%)\n",
    "2. Trenujemy różne modele na zbiorze _**trenującym**_\n",
    "3. Oceniamy modele na zbiorze _**walidacyjnym**_\n",
    "4. Wybieramy najlepszy\n",
    "5. Skuteczność podajemy na zbiorze  _**testowym**_\n",
    "\n",
    "---\n",
    "### Walidacja krzyżowa\n",
    "\n",
    "\n",
    "\n",
    "![Walidacja krzyżowa](xvi.png)\n",
    "\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "y = dum_df['cena_za_metr']\n",
    "X = dum_df.drop(['cena_za_metr', 'opis'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "scores = cross_val_score(LinearRegression(), X_train, y_train, cv=5)\n",
    "print(list(scores))\n",
    "print()\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
