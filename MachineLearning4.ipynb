{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Feature Engineering\n",
    "### - Ostateczny kształt `Pipeline`\n",
    "### - Problemy z trenowaniem modelu\n",
    "### - Materiały do dalszej nauki\n",
    "### - Projekt do realizacji\n",
    "\n",
    "---\n",
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The features you use influence more than everything else the result. \n",
    "# No algorithm alone, to my knowledge, can supplement the information gain given by correct feature engineering.\n",
    "# <div style=\"text-align: right\">— Luca Massaron Autor, Kaggle master</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coming up with features is difficult, time-consuming, requires expert knowledge.\n",
    "# \"_*Applied machine learning*_\" is basically feature engineering.\n",
    "# <div style=\"text-align: right\">— Andrew Ng</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Techniki Inżynierii Wymiarów\n",
    "### Liczby\n",
    "- Binaryzacja\n",
    "- Kubełkowanie (stała szerokość lub kwantyle)\n",
    "- Skalowanie \n",
    "  - wygładzanie (dodanie +1 - _*Wygładzanie Laplace'a*_)\n",
    "  - min-max \n",
    "  - logarytm \n",
    "  - standaryzacja (skalowanie o wariancję)\n",
    "  - NIE gubimy zer! (dane rzadkie w gęste przez np. odjęcie średniej)\n",
    "  - zaawansowane skalowanie: TF-IDF\n",
    "\n",
    "### Agregacje\n",
    "- sumy, średnie, wariancje, dalsze momenty (np. per kategoria)\n",
    "- przejście z liczb bezwzględnych na względne (np. w kategorii)\n",
    "- przejście na z wartości na rank (kolejność) \n",
    "\n",
    "\n",
    "### Kategorie\n",
    "- dummy encoding\n",
    "- feature hashing\n",
    "- redukcja wymiarów\n",
    "\n",
    "### Kategorie porządkowe np. daty\n",
    "- rozbicie na elementy (dzień, miesiąc, rok, kwartał, dzień tygodnia, dzień miesiąca)\n",
    "- przejście na zmienne biegunowe\n",
    "\n",
    "![Zmienne biegunowe](Picture1.png)\n",
    "\n",
    "---\n",
    "## Ostateczny kształt `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import log2\n",
    "\n",
    "data = pd.read_csv('adverts_29_04.csv', sep=';')\n",
    "\n",
    "data['cena_za_metr'] = data['Cena'] / data['Wielkość (m2)']\n",
    "data[\"log\"] = data['Wielkość (m2)'].apply(lambda x: log2(x))\n",
    "data['msc'] = data['Data dodania'].apply(lambda x: x[3:])\n",
    "\n",
    "data = data.dropna(subset=['cena_za_metr'])\n",
    "\n",
    "df = data.drop(['Cena', 'Data dodania'], axis=1)\n",
    "\n",
    "dum_df = pd.get_dummies(df, columns=['msc', 'Lokalizacja', 'Na sprzedaż przez', 'Rodzaj nieruchomości', 'Liczba pokoi', 'Liczba łazienek', 'Parking'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import sys\n",
    "import re\n",
    "import re\n",
    "\n",
    "splitter = re.compile(r'[^ąąćęńłóóśśżżź\\w]+')\n",
    "isnumber = re.compile(r'[0-9]')\n",
    "\n",
    "f = gzip.open('odm.txt.gz', 'rt', encoding='utf-8')\n",
    "dictionary = {}\n",
    "set_dict= set()\n",
    "\n",
    "for x in f:\n",
    "    t = x.strip().split(',')\n",
    "    tt = [ x.strip().lower() for x in t]\n",
    "    for w in tt:\n",
    "        set_dict.add(w)\n",
    "        dictionary[w]=tt[0]\n",
    "\n",
    "def lematize(w):\n",
    "    w = w.replace('ą','ą')\n",
    "    w = w.replace('ó','ó')\n",
    "    w = w.replace('ę','ę')\n",
    "    w = w.replace('ż','ż')\n",
    "    return dictionary.get(w,w)\n",
    "\n",
    "opis1 = dum_df['opis'][0]\n",
    "\n",
    "\n",
    "\n",
    "raw_corpus=[]\n",
    "n=0\n",
    "\n",
    "for i in dum_df.iterrows():\n",
    "    n+=1\n",
    "    l = list(splitter.split(i[1][1]))\n",
    "    raw_corpus.append(l)\n",
    "\n",
    "    \n",
    "all_words = []\n",
    "for t in raw_corpus:\n",
    "    all_words[0:0] = t\n",
    "\n",
    "words = {}\n",
    "for w in all_words:\n",
    "    rec = words.get(w.lower(), {'upper':0, 'lower': 0})\n",
    "    if w.lower()==w or w.upper()==w:\n",
    "        rec['lower'] = rec['lower'] +1\n",
    "    else: \n",
    "        rec['upper'] = rec['upper'] +1\n",
    "    words[w.lower()] = rec\n",
    "\n",
    "raw_stop_words = [ x for x in words.keys() if words[x]['upper']>=words[x]['lower']*4 ]   \n",
    "\n",
    "set_raw_stop_words = set(raw_stop_words)\n",
    "\n",
    "def preprocessing(opis, filter_raw=True, filter_dict=True):\n",
    "    opis = str(opis)\n",
    "    tokenized = splitter.split(opis)\n",
    "    l = list(tokenized)\n",
    "    l = [ x.lower() for x in l ]\n",
    "    l = [ x for x in l if len(x) > 2]\n",
    "    l = [ x for x in l if x.find('_') < 0]\n",
    "    l = [ x for x in l if isnumber.search(x) is None ]\n",
    "    if filter_raw: l = [ x for x in l if x not in set_raw_stop_words ]\n",
    "    if filter_dict: l = [ x for x in l if x in set_dict ]\n",
    "    l = [ lematize(x) for x in l ]\n",
    "    l = [ x for x in l if len(x) > 2]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opis1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessing(opis1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessing(opis1, filter_raw=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessing(opis1, filter_dict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessing(opis1, filter_raw=False, filter_dict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_df[\"opisTT\"] = dum_df[\"opis\"].apply(lambda x: ' '.join(preprocessing(x,filter_raw=True, filter_dict=True)))\n",
    "dum_df[\"opisTF\"] = dum_df[\"opis\"].apply(lambda x: ' '.join(preprocessing(x,filter_raw=True, filter_dict=False)))\n",
    "dum_df[\"opisFT\"] = dum_df[\"opis\"].apply(lambda x: ' '.join(preprocessing(x,filter_raw=False, filter_dict=True)))\n",
    "dum_df[\"opisFF\"] = dum_df[\"opis\"].apply(lambda x: ' '.join(preprocessing(x,filter_raw=False, filter_dict=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, RobustScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key=''):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "class ItemUnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keys=[]):\n",
    "        self.keys = keys\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict.drop(self.keys, axis=1)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "   ('union', \n",
    "        FeatureUnion(\n",
    "            transformer_list=[\n",
    "                ('table', \n",
    "                    Pipeline([\n",
    "                        ('selector1', ItemUnSelector(keys=['opis', 'opisTT', 'opisTF', 'opisFT', 'opisFF'])),\n",
    "                        ('scaler1', 'passthrough')\n",
    "                    ])\n",
    "                ),\n",
    "                ('description', \n",
    "                    Pipeline([\n",
    "                        ('selector2', ItemSelector()),\n",
    "                        ('tfidf', TfidfVectorizer()),\n",
    "                        ('best', TruncatedSVD()),\n",
    "                        ('scaler2', 'passthrough')\n",
    "                    ])\n",
    "                )\n",
    "            ]\n",
    "        )    \n",
    "\n",
    "   ),\n",
    "   ('regressor', \n",
    "        TransformedTargetRegressor()\n",
    "    )\n",
    "])\n",
    "\n",
    "parameters = parameters = {\n",
    "    'union__transformer_weights': [ { 'table': 3.0, 'description': 1.0}, { 'table': 2.0, 'description': 1.0}, { 'table': 1.0, 'description': 1.0}],\n",
    "\n",
    "    'union__description__best__n_components': (650, 700, 750),\n",
    "    'union__description__tfidf__min_df': (3, 4, 5),\n",
    "    'union__description__tfidf__binary': (True,False),\n",
    "    'union__description__selector2__key': ['opisTT', 'opisTF', 'opisFT', 'opisFF'] ,\n",
    "    \n",
    "    'union__table__scaler1': ['passthrough', StandardScaler(), Normalizer(), RobustScaler()],\n",
    "    'union__description__scaler2': ['passthrough', StandardScaler(), Normalizer(), RobustScaler(with_centering=False)],\n",
    "    \n",
    "    'regressor': [SVR(kernel='rbf', C=10000), SVR(kernel='linear', C=10000), GradientBoostingRegressor()] ,\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, verbose=1, cv=10, n_jobs=-1)\n",
    "\n",
    "\n",
    "y = dum_df['cena_za_metr']\n",
    "X = dum_df.drop(['cena_za_metr'], axis=1)\n",
    "\n",
    "t0 = time()\n",
    "grid_search.fit(X, y)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print(grid_search.cv_results_)\n",
    "print(grid_search.best_score_)\n",
    "print()\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problemy z trenowaniem modelu\n",
    "\n",
    "\n",
    "### To ile tych prób mamy ?\n",
    "\n",
    "- 3 zestawy wag `union`\n",
    "- 3 zestawy wymiarów SVD\n",
    "- 6 zestawów parametrów TF-IDF\n",
    "- 4 zbiory danych tekstowych\n",
    "- 4 mechanizmy skalowania części `table`\n",
    "- 4 mechanizmy skalowania części `description`\n",
    "- 3 regresory\n",
    "- 10 walidacji krzyżowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 * 3 * 6 * 4 * 4 * 4 * 3 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, RobustScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key=''):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "class ItemUnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keys=[]):\n",
    "        self.keys = keys\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict.drop(self.keys, axis=1)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "   ('union', \n",
    "        FeatureUnion(\n",
    "            transformer_list=[\n",
    "                ('table', \n",
    "                    Pipeline([\n",
    "                        ('selector1', ItemUnSelector(keys=['opis', 'opisTT', 'opisTF', 'opisFT', 'opisFF'])),\n",
    "                        ('scaler1', 'passthrough')\n",
    "                    ])\n",
    "                ),\n",
    "                ('description', \n",
    "                    Pipeline([\n",
    "                        ('selector2', ItemSelector()),\n",
    "                        ('tfidf', TfidfVectorizer()),\n",
    "                        ('best', TruncatedSVD()),\n",
    "                        ('scaler2', 'passthrough')\n",
    "                    ])\n",
    "                )\n",
    "            ]\n",
    "        )    \n",
    "\n",
    "   ),\n",
    "   ('regressor', \n",
    "        TransformedTargetRegressor()\n",
    "    )\n",
    "])\n",
    "\n",
    "parameters = parameters = {\n",
    "    'union__transformer_weights': [  { 'table': 1.0, 'description': 1.0}],\n",
    "\n",
    "    'union__description__best__n_components': (700,),\n",
    "    'union__description__tfidf__min_df': (3,),\n",
    "    'union__description__tfidf__binary': (True,),\n",
    "    'union__description__selector2__key': [ 'opisFF'] ,\n",
    "    \n",
    "    'union__table__scaler1': [ RobustScaler()],\n",
    "    'union__description__scaler2': [ RobustScaler(with_centering=False)],\n",
    "    \n",
    "    'regressor': [ GradientBoostingRegressor()] ,\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, verbose=1, cv=10, n_jobs=-1)\n",
    "\n",
    "\n",
    "y = dum_df['cena_za_metr']\n",
    "X = dum_df.drop(['cena_za_metr'], axis=1)\n",
    "\n",
    "t0 = time()\n",
    "grid_search.fit(X, y)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(f'Best score: {grid_search.best_score_}')\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print()\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secs = 3 * 3 * 6 * 4 * 4 * 4 * 3 * 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secs/(3600*24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Poprawa skuteczności\n",
    "### Więcej informacji\n",
    "- poziomo - więcej danych (skąd ? czy to nie zaburzy modelu ?)\n",
    "- pionowo - więcej wymiarów\n",
    "  - więcej danych (zdjęcia ?)\n",
    "  - więcej wymiarów - Feature Engineering\n",
    "---\n",
    "## Materiały do dalszej nauki\n",
    "- Udemy - https://www.udemy.com/course/introduction-to-data-science-using-python/\n",
    "- Udemy - https://www.udemy.com/course/python-scrapy-for-beginners/\n",
    "- edX - https://www.edx.org/course/introduction-to-python-for-data-science-2\n",
    "- Coursera - IBM https://www.coursera.org/learn/python-for-applied-data-science-ai\n",
    "- Coursera - Stanford Machine Learning https://www.coursera.org/learn/machine-learning\n",
    "\n",
    "### Tego jest dużo ...\n",
    "https://www.forbes.com/sites/bernardmarr/2020/02/24/the-9-best-free-online-data-science-courses-in-2020/\n",
    "https://www.dataquest.io/blog/free-books-learn-data-science/\n",
    "100+ - https://www.learndatasci.com/free-data-science-books/\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Temat Projektu\n",
    "\n",
    "- Pobierz dane (`Scrapy`, `requests` ...) - ok. 1000 rekordów (im więcej, tym lepiej)\n",
    "- Przygotuj dane do analizy (`Beautiful Soup`, `lxml`) \n",
    "- Zbuduj `Pipeline`\n",
    "- Wytrenuj jak najlepszy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
